{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://zai-org-glm-4-5-space.hf.space ✔\n",
      "([{'role': 'user', 'metadata': None, 'content': 'what is genai!!', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"<div style='margin:0.5em 0; white-space: pre-wrap; line-height:1.6;'><br>- **Image Generation**: DALL-E, Midjourney, Stable Diffusion<br>- **Code Generation**: GitHub Copilot, CodeLlama<br>- **Audio Generation**: MusicLM, ElevenLabs<br><br>## How It Works<br><br>GenAI typically uses deep learning models like neural networks to learn patterns from training data and then generates new content that follows similar patterns while being unique.<br><br>These technologies are rapidly evolving and transforming many industries by automating creative tasks and enhancing human capabilities.</div>\", 'options': None}], '')\n"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"zai-org/GLM-4.5-Space\")\n",
    "result = client.predict(\n",
    "\t\tmsg=\"what is genai!!\",\n",
    "\t\tsys_prompt=\"You are a helpful assistant whose only job is to answer the users questions as best you can in polite and professional manner.You only respond in markdown.\",\n",
    "\t\t# thinking_enabled=True,\n",
    "\t\ttemperature=1,\n",
    "\t\tapi_name=\"/chat_wrapper\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6db8803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<details open><summary style='cursor:pointer;font-weight:bold;color:#007acc;'>Thinking</summary><div style='color:#555555;line-height:1.6;padding:15px;border-left:4px solid #007acc;margin:10px 0;background-color:#f0f7ff;border-radius:4px;'>The user is asking about &quot;GenAI&quot;. I believe they&#x27;re referring to &quot;Generative AI&quot;, which is a type of artificial intelligence that can generate new content (like text, images, audio, or synthetic data) rather than just analyzing or classifying existing data.<br><br>I&#x27;ll provide a clear, professional explanation of what Generative AI is, how it works, and some examples of it in use. I&#x27;ll format my response in markdown as requested.<br><br>Let me structure my response:<br>1. Definition of Generative AI<br>2. How it works (brief explanation)<br>3. Common types of Generative AI<br>4. Examples and applications<br>5. Current significance in the technology landscape</div></details><div style='margin:0.5em 0; white-space: pre-wrap; line-height:1.6;'># What is Generative AI?<br><br>Generative AI (GenAI) refers to artificial intelligence systems capable of creating new content—such as text, images, audio, video, or synthetic data—rather than simply analyzing or classifying existing information.<br><br>## How It Works<br><br>Generative AI models learn patterns from vast amounts of training data and then use this knowledge to generate original content that resembles the training data. These models typically use advanced techniques like:<br><br>- **Neural networks** (particularly transformer architectures)<br>- **Deep learning** approaches<br>- **Large language models** (for text generation)<br>- **Diffusion models** and **GANs** (for image generation)<br><br>## Common Types of Generative AI<br><br>- **Text generators**: GPT-4, Claude, Llama, etc.<br>- **Image generators**: DALL-E, Midjourney, Stable Diffusion<br>- **Code generators**: GitHub Copilot, Amazon CodeWhisperer<br>- **Audio generators**: MusicLM, ElevenLabs<br>- **Video generators**: Sora, Runway<br><br>## Applications<br><br>Generative AI is being used across numerous industries for:<br>- Content creation and marketing<br>- Software development assistance<br>- Customer service chatbots<br>- Research and data analysis<br>- Creative arts and design<br>- Education and training<br><br>This technology represents a significant shift in how humans interact with AI, moving from analytical capabilities to creative collaboration.</div>\n"
     ]
    }
   ],
   "source": [
    "print(result[-2][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b01bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac32d1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThinkingThe user is asking about \"GenAI\". I believe they're referring to \"Generative AI\", which is a type of artificial intelligence that can generate new content (like text, images, audio, or synthetic data) rather than just analyzing or classifying existing data.I'll provide a clear, professional explanation of what Generative AI is, how it works, and some examples of it in use. I'll format my response in markdown as requested.Let me structure my response:1. Definition of Generative AI2. How it works (brief explanation)3. Common types of Generative AI4. Examples and applications5. Current significance in the technology landscape# What is Generative AI?Generative AI (GenAI) refers to artificial intelligence systems capable of creating new content—such as text, images, audio, video, or synthetic data—rather than simply analyzing or classifying existing information.## How It WorksGenerative AI models learn patterns from vast amounts of training data and then use this knowledge to generate original content that resembles the training data. These models typically use advanced techniques like:- **Neural networks** (particularly transformer architectures)- **Deep learning** approaches- **Large language models** (for text generation)- **Diffusion models** and **GANs** (for image generation)## Common Types of Generative AI- **Text generators**: GPT-4, Claude, Llama, etc.- **Image generators**: DALL-E, Midjourney, Stable Diffusion- **Code generators**: GitHub Copilot, Amazon CodeWhisperer- **Audio generators**: MusicLM, ElevenLabs- **Video generators**: Sora, Runway## ApplicationsGenerative AI is being used across numerous industries for:- Content creation and marketing- Software development assistance- Customer service chatbots- Research and data analysis- Creative arts and design- Education and trainingThis technology represents a significant shift in how humans interact with AI, moving from analytical capabilities to creative collaboration.\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(result[-2][-1]['content'], 'html.parser')\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b39957f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://multimodalart-nano-banana.hf.space ✔\n"
     ]
    },
    {
     "ename": "AppError",
     "evalue": "Access Denied. This service is for PRO users only.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAppError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgradio_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Client, handle_file\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m Client(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultimodalart/nano-banana\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello!!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/unified_image_generator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/PycharmProjects/llm_engineering/llms/lib/python3.10/site-packages/gradio_client/client.py:500\u001b[0m, in \u001b[0;36mClient.predict\u001b[0;34m(self, api_name, fn_index, headers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03mCalls the Gradio API and returns the result (this is a blocking call). Arguments can be provided as positional arguments or as keyword arguments (latter is recommended).\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m    >> 9.0\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_fn_index(api_name, fn_index)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m--> 500\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/llm_engineering/llms/lib/python3.10/site-packages/gradio_client/client.py:1605\u001b[0m, in \u001b[0;36mJob.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;124;03m    Return the result of the call that the future represents. Raises CancelledError: If the future was cancelled, TimeoutError: If the future didn't finish executing before the given timeout, and Exception: If the call raised then that exception will be raised.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;124;03m        >> 9\u001b[39;00m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/PycharmProjects/llm_engineering/llms/lib/python3.10/site-packages/gradio_client/client.py:1209\u001b[0m, in \u001b[0;36mEndpoint.make_end_to_end_fn.<locals>._inner\u001b[0;34m(*data)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert_empty_state(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[1;32m   1208\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_input_files(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m-> 1209\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_predictions(\u001b[38;5;241m*\u001b[39mpredictions)\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# Append final output only if not already present\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# for consistency between generators and not generators\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/llm_engineering/llms/lib/python3.10/site-packages/gradio_client/client.py:1329\u001b[0m, in \u001b[0;36mEndpoint.make_predict.<locals>._predict\u001b[0;34m(*data)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1328\u001b[0m         message \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AppError(message\u001b[38;5;241m=\u001b[39mmessage, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresult)\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mAppError\u001b[0m: Access Denied. This service is for PRO users only."
     ]
    }
   ],
   "source": [
    "from gradio_client import Client, handle_file\n",
    "\n",
    "client = Client(\"multimodalart/nano-banana\")\n",
    "result = client.predict(\n",
    "\t\tprompt=\"Hello!!\",\n",
    "\t\timages=[],\n",
    "\t\tapi_name=\"/unified_image_generator\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "249b88af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://ameerazam08-gemini-2-5-flash-nano-banana.hf.space ✔\n"
     ]
    },
    {
     "ename": "AppError",
     "evalue": "The upstream Gradio app has raised an exception but has not enabled verbose error reporting. To enable, set show_error=True in launch().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAppError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgradio_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Client, handle_file\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m Client(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mameerazam08/Gemini-2.5-Flash-Nano-Banana\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43muploaded_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../../Downloads/Linear-Brand-Assets/linear-app-icon.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello!!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello!!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/process_image_and_prompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/PycharmProjects/llm_engineering/llms/lib/python3.10/site-packages/gradio_client/client.py:500\u001b[0m, in \u001b[0;36mClient.predict\u001b[0;34m(self, api_name, fn_index, headers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03mCalls the Gradio API and returns the result (this is a blocking call). Arguments can be provided as positional arguments or as keyword arguments (latter is recommended).\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m    >> 9.0\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_fn_index(api_name, fn_index)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m--> 500\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/llm_engineering/llms/lib/python3.10/site-packages/gradio_client/client.py:1605\u001b[0m, in \u001b[0;36mJob.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;124;03m    Return the result of the call that the future represents. Raises CancelledError: If the future was cancelled, TimeoutError: If the future didn't finish executing before the given timeout, and Exception: If the call raised then that exception will be raised.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;124;03m        >> 9\u001b[39;00m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/PycharmProjects/llm_engineering/llms/lib/python3.10/site-packages/gradio_client/client.py:1209\u001b[0m, in \u001b[0;36mEndpoint.make_end_to_end_fn.<locals>._inner\u001b[0;34m(*data)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert_empty_state(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[1;32m   1208\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_input_files(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m-> 1209\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_predictions(\u001b[38;5;241m*\u001b[39mpredictions)\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# Append final output only if not already present\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# for consistency between generators and not generators\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/llm_engineering/llms/lib/python3.10/site-packages/gradio_client/client.py:1323\u001b[0m, in \u001b[0;36mEndpoint.make_predict.<locals>._predict\u001b[0;34m(*data)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AppError(\n\u001b[1;32m   1324\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe upstream Gradio app has raised an exception but has not enabled \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1325\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose error reporting. To enable, set show_error=True in launch().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1326\u001b[0m         )\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1328\u001b[0m         message \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAppError\u001b[0m: The upstream Gradio app has raised an exception but has not enabled verbose error reporting. To enable, set show_error=True in launch()."
     ]
    }
   ],
   "source": [
    "from gradio_client import Client, handle_file\n",
    "\n",
    "client = Client(\"ameerazam08/Gemini-2.5-Flash-Nano-Banana\")\n",
    "result = client.predict(\n",
    "\t\tuploaded_files=\"../../../Downloads/Linear-Brand-Assets/linear-app-icon.png\",\n",
    "\t\tprompt=\"Hello!!\",\n",
    "\t\tgemini_api_key=\"Hello!!\",\n",
    "\t\tapi_name=\"/process_image_and_prompt\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d1ca55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
